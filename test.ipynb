{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74fed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a74ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str):\n",
    "    with open(\"prompt/system_prompt.txt\", 'r') as f:\n",
    "        system_prompt = f.read()\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model='deepseek-r1:8b',\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"system\", \"content\": system_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    content = response['message']['content']\n",
    "    # content = content[::-1]\n",
    "    # start = content.find(''.join(reversed('</answer>'))) + len(''.join(reversed('?</answer>')))\n",
    "    # end = start + content[start:].find(''.join(reversed('<answer>')))\n",
    "    # content = content[start:end]\n",
    "    # content = content[::-1]\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a4acb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = generate(\"\"\"\n",
    "                     A\n",
    "                     \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4393a70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, the system prompt says I'm the INTERVIEWER, and I must follow a sequence strictly. The type of interview isn't specified here; it's just given as an example that there are two types: generic (HR) or technical.\n",
      "\n",
      "In this user message, it seems like they're providing instructions for how to format my responses, but in the context, it might be part of the conversation. Looking back:\n",
      "\n",
      "User said: \"A\" – which is probably a response from a candidate or something similar. But according to the rules, I need to start by greeting if it's a generic interview.\n",
      "\n",
      "The system prompt says: \"MUST -> The type of interview will be mentioned in starting and there are two type of interviews first is generic (HR) and second is technical.\"\n",
      "\n",
      "But in this interaction, no type is specified. Perhaps I need to assume the type or ask for clarification? No, let's read carefully.\n",
      "\n",
      "The user message is: \"A\" – that might be incomplete or a placeholder. Looking at the conversation history:\n",
      "\n",
      "There was a previous system prompt, but then the human said something like \"Human: [some text]\", and now it's \"Human: A\".\n",
      "\n",
      "This could be a misinterpretation. In the initial setup, there might be an indication of the interview type.\n",
      "\n",
      "Perhaps this is part of the candidate's response or input. Let me think about how to proceed.\n",
      "\n",
      "According to the rules:\n",
      "\n",
      "- I must always put my question inside <answer> tags.\n",
      "\n",
      "- Questions should be between 60-100 characters for generic interviews, but for technical, it says MCQ with options in <option> tags.\n",
      "\n",
      "The user message is just \"A\", which might mean that this is the candidate's first response or something. But I need to start by asking a question based on the interview type.\n",
      "\n",
      "I think there might be confusion because the system prompt mentions two types: generic and technical, but doesn't specify which one to use here. However, in the example given, it says \"if the interview is generic (HR): first greet...\", so probably for this interaction, since no specific role is given, I should start with a generic greeting.\n",
      "\n",
      "But let's see the sequence:\n",
      "\n",
      "- The type of interview will be mentioned in starting – but what does that mean? Perhaps the human will specify or it's implied.\n",
      "\n",
      "In this case, the human has said nothing about the interview type. Maybe I need to begin by asking for clarification on the interview type?\n",
      "\n",
      "No, the system prompt says: \"MUST -> The type of interview will be mentioned in starting\" – but who mentions it? It might mean that the context provides it, or perhaps it's part of my role.\n",
      "\n",
      "Reading the system prompt again:\n",
      "\n",
      "\"You are the INTERVIEWER. You will follow this sequence STRICTLY and ONESTEP AT A TIME: MUST -> The type of interview will be mentioned in starting...\"\n",
      "\n",
      "It doesn't specify how the type is determined. But then it says \"based on type of interview\" for questions.\n",
      "\n",
      "Perhaps I should assume a default or look for context. In the initial message, there might be an indication, but here the human just said \"A\".\n",
      "\n",
      "Maybe this is a test where I need to start with a greeting if no type is specified.\n",
      "\n",
      "But let's check the technical part: it says always generate MCQ questions if technical interview.\n",
      "\n",
      "I think I need to clarify what kind of interview we're conducting first.\n",
      "\n",
      "Best approach: Since the system prompt doesn't specify, and in generic case, I should greet first, I'll start with a greeting assuming it's a generic interview for now. If not, adjust later.\n",
      "\n",
      "Also, remember that all interactions are based on the type, so once I know the type, stick to it.\n",
      "\n",
      "Now, user responded with \"A\", which might be their answer or something else. But in this format, perhaps it's incomplete.\n",
      "\n",
      "Looking at the way human messages: Human: [text], and system responds accordingly.\n",
      "\n",
      "In the first message, human said nothing, just a prompt? No:\n",
      "\n",
      "- System: Initial setup\n",
      "\n",
      "- Human: Some text?\n",
      "\n",
      "Actually, from the conversation history provided:\n",
      "\n",
      "Human: \"A\"\n",
      "\n",
      "And before that, there was some context about interview types.\n",
      "\n",
      "I think I should proceed by asking my first question based on the type.\n",
      "\n",
      "But what is the type here? Not specified.\n",
      "\n",
      "Perhaps I need to ask for the interview type in my response.\n",
      "\n",
      "Let's read the system prompt carefully.\n",
      "\n",
      "System says: MUST -> The type of interview will be mentioned in starting – meaning that at the beginning, it should be set.\n",
      "\n",
      "Then, \"all interaction with candidate will be based and must based on type of interview.\"\n",
      "\n",
      "But here, no mention. Maybe I need to elicit this from the candidate or assume.\n",
      "\n",
      "To avoid errors, perhaps my first response should confirm the interview type.\n",
      "\n",
      "Since the technical part specifies MCQ questions only if technical, for generic, it's open-ended questions about hobbies, etc.\n",
      "\n",
      "Let me structure my response step by step as per rules.\n",
      "\n",
      "Rules: A. NEVER ask more than one question at once. B. NEVER include your reasoning or thinking process. C. ALWAYS wait for the candidate’s reply after each question. D. Always ask related questions based on previous answers and job description/experience, but initially it's generic-based until type is known? E. Base questions on interview type and don't change till end.\n",
      "\n",
      "But in this case, I need to start with a greeting if generic, or define the technical format first.\n",
      "\n",
      "Perhaps for the first question, since no type specified, I should use the generic method as per the initial description.\n",
      "\n",
      "Let me think about how standard interviews work. Usually, an interviewer starts by asking basic questions.\n",
      "\n",
      "In this system prompt, it says: \"MUST -> The type of interview will be mentioned in starting\" – but not what to say if not.\n",
      "\n",
      "Another part: \"if the interview is generic (HR): first Greet the candidate warmly asking how they are?\"\n",
      "\n",
      "So probably, for a generic interview, I should greet them with that question.\n",
      "\n",
      "For technical, it's different.\n",
      "\n",
      "But here, user just said \"A\", which might be indicating something or their response.\n",
      "\n",
      "Perhaps this is not part of the actual interview sequence. Let's see the full message:\n",
      "\n",
      "Human: \"A\"\n",
      "\n",
      "And system prompt has instructions.\n",
      "\n",
      "I think in this context, I need to treat the human as the candidate and start with a question based on assumed type.\n",
      "\n",
      "To follow strictly, since no type is mentioned, perhaps default to generic HR questions.\n",
      "\n",
      "But let's make it simple. My first response should be within <answer> tags asking one question at a time.\n",
      "\n",
      "Also, for technical interview, I must use MCQ format with options in <option> tags.\n",
      "\n",
      "Since the human responded with \"A\", which could be an answer or just input, I need to ask my next question based on that if it were part of an ongoing interview.\n",
      "\n",
      "But this is likely the start. Let's assume that for now, we're conducting a generic HR interview because no other type was specified in the system prompt except the example.\n",
      "\n",
      "Perhaps I should begin by asking what type of interview they expect or something.\n",
      "\n",
      "Best to stick with the rules: put question inside <answer> tags, and wait for reply.\n",
      "\n",
      "So, my first action is to output one question inside <answer>.\n",
      "\n",
      "But what type? Not defined. Let's use generic HR greeting as per prompt.\n",
      "\n",
      "System says: \"You will follow this sequence STRICTLY and ONESTEP AT A TIME\"\n",
      "\n",
      "The sequence isn't fully described; it just mentions the two types.\n",
      "\n",
      "Perhaps I need to have a starting point.\n",
      "\n",
      "I think for now, since no specific context, I'll start with a generic HR interview by greeting them.\n",
      "\n",
      "Then, after they respond, proceed with more questions based on that and their job description (which is not given yet).\n",
      "\n",
      "But in this case, human just said \"A\", so maybe it's incomplete or needs clarification.\n",
      "\n",
      "Perhaps the user meant to say something like \"Human: I am interested in the position\" but wrote only \"A\".\n",
      "\n",
      "I should respond by asking a question based on standard HR practices.\n",
      "\n",
      "Let me formulate my response.\n",
      "</think>\n",
      "<answer>Question 1: How are you feeling today, and what questions do you have about this role or our company?</answer>\n",
      "\n",
      "Now, wait for the candidate's reply before proceeding.\n"
     ]
    }
   ],
   "source": [
    "print(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a3548",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = input(\"Name\")\n",
    "role = input(\"Role\")\n",
    "experience  = input(\"Years of Exp\")\n",
    "prompt = str({'Name': {name}, 'role': {role}, 'years of experience': {experience}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b698da45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bfd5501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': {'Mohit'}, 'role': {'DS'}, 'years of experience': {'0-1'}}\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9850445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatetechnical(prompt: str):\n",
    "    with open(\"prompt/system_prompt_mcq.txt\", mode='r', encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        system_prompt = f.read()\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='phi3:3.8b',\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"system\", \"content\": system_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        content = response['message']['content']\n",
    "\n",
    "        tags = (\"question\", \"A\", \"B\", \"C\", \"D\")\n",
    "        qna = {}\n",
    "        for tag in tags:\n",
    "            start_tag = \"<\" + tag + \">\"\n",
    "            end_tag = \"</\" + tag + \">\"\n",
    "            material = content[::-1]\n",
    "            \n",
    "            start_idx = material.find(''.join(reversed(end_tag))) + len(''.join(reversed(end_tag)))\n",
    "            end_idx = start_idx + material[start_idx:].find(''.join(reversed(start_tag)))\n",
    "            item = material[start_idx:end_idx]\n",
    "            item = material[::-1]\n",
    "\n",
    "            qna[tag] = item\n",
    "\n",
    "        return qna\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: Failed to generate question: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8d2e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatetechnical(prompt: str):\n",
    "    with open(\"prompt/system_prompt_mcq.txt\", mode='r', encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        system_prompt = f.read()\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='phi3:3.8b',\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"system\", \"content\": system_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        content = response['message']['content']['question']\n",
    "\n",
    "        tags = (\"question\", \"A\", \"B\", \"C\", \"D\")\n",
    "        qna = {}\n",
    "        material = content[::-1]        \n",
    "        for tag in tags:\n",
    "            if tag != \"question\":\n",
    "                start_tag = \"<option>\" + tag + \")\"\n",
    "                end_tag = \"</option>\"\n",
    "                material = content[::-1]\n",
    "            else:    \n",
    "                start_tag = \"<question>\"\n",
    "                end_tag = \"</question>\"\n",
    "                material = content[::-1]\n",
    "\n",
    "            start_idx = material.find(''.join(reversed(end_tag))) + len(''.join(reversed(end_tag)))\n",
    "            end_idx = start_idx + material[start_idx:].find(''.join(reversed(start_tag)))\n",
    "            item = material[start_idx:end_idx]\n",
    "            item = material[::-1]\n",
    "\n",
    "            qna[tag] = item\n",
    "\n",
    "        return qna\n",
    "    except Exception as e:\n",
    "        return f\"Error: Failed to generate question: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e80fd1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generatetechnical(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2a0d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompt/system_prompt_mcq.txt\", mode='r', encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    system_prompt = f.read()\n",
    "    \n",
    "response = ollama.chat(\n",
    "            model='deepseek-r1:8b',\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"system\", \"content\": system_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "content = response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "316e5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna = {}\n",
    "tags = (\"question\", \"options\")\n",
    "material = content[::-1]     \n",
    "for tag in tags:\n",
    "    start_tag = \"<\" + tag + \">\"\n",
    "    end_tag = \"</\" + tag + \">\"\n",
    "    start_idx = material.find(''.join(reversed(end_tag))) + len(''.join(reversed(end_tag)))\n",
    "    end_idx = start_idx + material[start_idx:].find(''.join(reversed(start_tag)))\n",
    "    item = material[start_idx:end_idx]\n",
    "    item = item[::-1]\n",
    "    qna[tag] = item\n",
    "\n",
    "options = (\"A:\", \"B:\", \"C:\", \"D:\")\n",
    "indices = []\n",
    "for option in options:\n",
    "    indices.append(qna[\"options\"].find(option))\n",
    "indices.append(len(qna[\"options\"]) - 1)\n",
    "answers = []\n",
    "for i in range(4):\n",
    "    answers.append(qna[\"options\"][indices[i]: indices[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "613dd27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A: Regression predicts continuous values; classification predicts discrete categories.  \\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb2c28a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nFirst, I am considering that I'm the interviewer. The user has provided their name (Mohit), role (Data Science Intern), and years of experience (0-1). That's clear enough.\\n\\nSince this is an entry-level position with 0-1 year experience, my questions should focus on fundamentals in data science. Key areas include basic concepts like machine learning algorithms, statistics, Python libraries for data manipulation (e.g., Pandas, NumPy), and perhaps some SQL basics since it's common in data roles.\\n\\nNow, I need to ask one multiple-choice question at a time. Each question must be concise: 60-100 characters excluding tags. Options should be exactly four, with only one correct answer marked internally.\\n\\nI'll start by asking a fundamental question related to their role. For example, something about regression vs. classification in machine learning, which is core for data science interns.\\n\\nLet me think of the first question:\\n\\nQuestion: What is the main difference between regression and classification tasks? Keep it under 100 characters.\\n\\nOption A: Regression predicts continuous values; classification predicts discrete categories.\\n\\nOption B: Classification uses linear models; regression does not.\\n\\nOption C: Regression involves feature scaling; classification does not need it.\\n\\nOption D: Classification requires cross-validation; regression never does.\\n\\nI know that option A is correct. So, I'll mark that internally and present only the options without indicating correctness to the candidate.\\n\\nAlso, ensure clarity and focus. My question should be straightforward.\\n\\nAfter each response, I wait for a valid one before proceeding.\\n\\nFinally, after 20 questions, I provide a summary of correct answers and qualitative assessment.\\n</think>\\n<question>What is the main difference between regression and classification tasks?</question>\\n<options>A: Regression predicts continuous values; classification predicts discrete categories.  \\nB: Classification uses linear models; regression does not.  \\nC: Regression involves feature scaling; classification does not need it.  \\nD: Classification requires cross-validation; regression never does.</options>\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ea800",
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d613a2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an interviewer for Mohit, who is currently working as a Data Science Intern with no more than one year of experience:\\n\\n<question>Which library in Python would you use primarily for data manipulation and analysis?</question>\\n<options><option>A) Matplotlib - Plotting graphs.</option>\\n  <option>B) NumPy - Array operations.</option>\\n  <option>C) Pandas - Data structures and data analysis tools (Correct Answer).</option>\\n  <option>D) Scikit-learn - Machine learning models.</option></options>\\n\\n(Awaiting candidate\\'s response...)\\n\\nIf the correct answer is provided: \"3/20\" or similar. Evaluate his strength in data manipulation and analysis, suggesting further discussion on Python for Data Science to assess deeper understanding of tools like Pandas within a technical interview setting based on Mohit’s performance throughout all 20 questions covering coding concepts, tool usage, problem-solving abilities specific to the role.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ddb38288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatetechnical0(prompt: str):\n",
    "    with open(\"prompt/system_prompt_mcq.txt\", mode='r', encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        system_prompt = f.read()\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='phi3:3.8b',\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"system\", \"content\": system_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        content = response['message']['content']\n",
    "\n",
    "        qna = {}\n",
    "        tags = (\"question\", \"options\")\n",
    "        material = content[::-1]     \n",
    "        for tag in tags:\n",
    "            start_tag = \"<\" + tag + \">\"\n",
    "            end_tag = \"</\" + tag + \">\"\n",
    "            start_idx = material.find(''.join(reversed(end_tag))) + len(''.join(reversed(end_tag)))\n",
    "            end_idx = start_idx + material[start_idx:].find(''.join(reversed(start_tag)))\n",
    "            item = material[start_idx:end_idx]\n",
    "            item = item[::-1]\n",
    "            qna[tag] = item\n",
    "\n",
    "        options = (\"A.\", \"B.\", \"C.\", \"D.\")\n",
    "        indices = []\n",
    "        for option in options:\n",
    "            indices.append(qna[\"options\"].find(option))\n",
    "        indices.append(len(qna[\"options\"]) - 1)\n",
    "        answers = [qna[\"question\"]]\n",
    "        for i in range(4):\n",
    "            answers.append(qna[\"options\"][indices[i]: indices[i+1]])\n",
    "\n",
    "\n",
    "        return content, answers\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: Failed to generate question: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "46a31919",
   "metadata": {},
   "outputs": [],
   "source": [
    "response0 = generatetechnical0(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "33359a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "facaaf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n    \"question\": \"<response>What is Pandas used for in Python?\",\\n    \"options\": {\\n        \"A\": \"Data manipulation and analysis.\",\\n        \"B\": \"Web page design.\",\\n        \"C\": \"Mobile app development.\",\\n        \"D\": \"Network security protocols.\"\\n    },\\n    \"answer\": \"A\"\\n}\\n```'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6dd5989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatetechnical(prompt: str):\n",
    "    with open(\"prompt/system_prompt_mcq.txt\", mode='r', encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        system_prompt = f.read()\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='deepseek-r1:8b',\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"system\", \"content\": system_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        content = response['message']['content']\n",
    "\n",
    "        qna = {}\n",
    "        tags = (\"question\", \"options\")\n",
    "        material = content[::-1]     \n",
    "        for tag in tags:\n",
    "            start_tag = \"<\" + tag + \">\"\n",
    "            end_tag = \"</\" + tag + \">\"\n",
    "            start_idx = material.find(''.join(reversed(end_tag))) + len(''.join(reversed(end_tag)))\n",
    "            end_idx = start_idx + material[start_idx:].find(''.join(reversed(start_tag)))\n",
    "            item = material[start_idx:end_idx]\n",
    "            item = item[::-1]\n",
    "            qna[tag] = item\n",
    "\n",
    "        options = (\"A.\", \"B.\", \"C.\", \"D.\")\n",
    "        indices = []\n",
    "        for option in options:\n",
    "            indices.append(qna[\"options\"].find(option))\n",
    "        indices.append(len(qna[\"options\"]) - 1)\n",
    "        answers = [qna[\"question\"]]\n",
    "        for i in range(4):\n",
    "            answers.append(qna[\"options\"][indices[i]: indices[i+1]])\n",
    "\n",
    "\n",
    "        return content, answers\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: Failed to generate question: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eef388",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generatetechnical(prompt)\n",
    "start_idx = response[0].find(\"</think>\") + len(\"</think>\")\n",
    "a = ''.join(response[0][start_idx:].split('\\n'))\n",
    "final = json.loads(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3ba4ccd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'To load data from a CSV file into a Pandas DataFrame, which method do you use?',\n",
       " 'options': {'A': 'pd.read_csv()',\n",
       "  'B': 'numpy.loadtxt()',\n",
       "  'C': 'pandas.import_csv()',\n",
       "  'D': \"dataframe.load('csv')\"},\n",
       " 'answer': 'A'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b43a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "\n",
    "\n",
    "def generatetechnical(prompt: str):\n",
    "    with open(\"prompt/system_prompt_mcq.txt\", mode='r', encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        system_prompt = f.read()\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model='deepseek-r1:8b',\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"system\", \"content\": system_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    content = response['message']['content']\n",
    "    start_idx = content.find(\"</think>\") + len(\"</think>\")\n",
    "    a = ''.join(content[start_idx:].split('\\n'))\n",
    "    final = json.loads(a)\n",
    "    return content, final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "151e729e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "490bad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatetechnical(prompt: str):\n",
    "    with open(\"prompt/system_prompt_mcq.txt\", mode='r', encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        system_prompt = f.read()\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='deepseek-r1:8b',\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"system\", \"content\": system_prompt}\n",
    "            ]\n",
    "        )    \n",
    "        content = response['message']['content']\n",
    "        start_idx = content.find(\"</think>\") + len(\"</think>\")\n",
    "        a = ''.join(content[start_idx:].split('\\n'))\n",
    "        final = json.loads(a)\n",
    "        return final\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: Failed to generate mcq (error while generating): {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df499559",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = input(\"Name\")\n",
    "role = input(\"Role\")\n",
    "experience  = input(\"Years of Exp\")\n",
    "prompt = str({'Name': {name}, 'role': {role}, 'years of experience': {experience}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc7ccf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generatetechnical(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dbf12b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'In machine learning, what is the primary goal of supervised learning?',\n",
       " 'options': {'A': 'To predict unknown future events based on learned patterns from labeled data',\n",
       "  'B': 'To cluster unlabeled data into groups without predefined categories',\n",
       "  'C': 'To find relationships between variables using statistical methods',\n",
       "  'D': 'To train a model to generate outputs for inputs it has never seen, but with no guidance'},\n",
       " 'answer': 'A'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37dbfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastapienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
